# 敵対的摂動による分類モデルの転移可能な脆弱性評価
## 概要
本リポジトリでは，画像分類タスクに対する**敵対的摂動（Adversarial Perturbation）の生成**と，その**転移可能性**に関する検証を行う．  
敵対的生成とは，分類や回帰といった機械学習モデルの出力を**意図的に誤らせる入力改変**であり，セキュリティやAIに関する信頼性の観点から，近年大きな注目を集めている.特に本リポジトリでは入力画像に対して**人間の認知では気づきにくい微小な変化**を加えることで，モデルの予測を誤らせる攻撃手法について検討を行う．   

敵対的摂動の多くは， **攻撃対象モデルの構造や勾配情報を前提とした"ホワイトボックス攻撃"を想定しているが，異なる構造のモデル間でも攻撃が有効に作用する"転移可能性"** の存在が知られている．これはモデルの構造を知りえない攻撃者にとって極めて都合の良い性質であり，**モデル構造非依存のセキュリティ上の深刻なリスク**とみなされている．

このような問題意識のもと，本リポジトリでは以下の点に焦点をあて，検証・考察を行う：
- 敵対的摂動の代表手法の実装と比較
- 異なる構造の分類モデル間での**攻撃転移性の定量的検証**
- 敵対的摂動の**局所領域制限（Local UAP）による現実的攻撃ケースの再現
- 攻撃手法の想定ユースケースに関する検討
- **攻撃の転移性が生じる要因についての理論的考察**
- **小規模モデルを用いた大規模モデルへの強力な転移攻撃**の実現可能性に関する考察


敵対事例の生成方法としては大別して以下の3つのアプローチを試みた: 
- FGSM  
    モデルの損失関数に対する勾配に沿って，1ステップで摂動を生成するシンプルかつ高速な攻撃手法．
- UAP  
    訓練データ全体に共通して適用できる汎用的な摂動ベクトル（行列）を学習し，タスクレベルでの誤分類を誘発する.
- 局所的UAP（Local UAP）  
    画像の一部領域（例：左上の8x8領域など）のみにUAPに基づく改変をすることで，現実世界における「ステッカー攻撃」などに対応．

※ 各攻撃方法の理論的詳細や想定ユースケースについては[こちら]()を参照．  
※ なお，本リポジトリでは**CIFAR-10**をデータに対する多クラス分類タスクを用いて検証を行った．

## モデル及びデータの構成
本リポジトリでは，CIFAR-10データセットに対する分類タスクを対称に以下の3種類の異なる画像分類モデルを使用した．各モデルに対して攻撃を加えるとともに，**転移性の定量評価**を行った
### 使用データセット（CIFAR-10）
- CIFAR-10
    - サイズ：32x32のRGB画像（チャネル数 = 3）
    - クラス数：10（airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck）
    - 学習用画像：50,000枚 / テスト画像：10,000枚

### モデル構成一覧
| モデル名        | 概要                                   | 主な用途（本リポジトリ内）        |
| ----------- | ------------------------------------ | -------------------- |
| `CNN_small` | 訓練データの内，10,000枚の画像を用いた小規模データのモデル   | 軽量な敵対摂動の訓練・転移元モデル    |
| `CNN`       | 訓練データをすべて使用したCNNモデル             | 汎用的な敵対摂動の訓練元・評価対象モデル |
| `ResNet18`  | torchvision実装のResNet構造．残差接続による深層分類器． | 高精度モデルとしての評価対象（転移先）  |


## 評価結果と転移性の検証
